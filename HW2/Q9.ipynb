{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cee03cd",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0642d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as npla\n",
    "import scipy.linalg as spla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f31a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving(i,j,action):\n",
    "    # action direction : upwards=2, downwards=3, left=1, right=0\n",
    "    newi = 0\n",
    "    newj = 0\n",
    "    # moving into terminating state\n",
    "    if i==0 and j==1 and action==1:\n",
    "        newi = 0\n",
    "        newj = 0\n",
    "    elif i==1 and j==0 and action==2:\n",
    "        newi = 0\n",
    "        newj = 0\n",
    "    elif i==2 and j==3 and action==3:\n",
    "        newi = 3\n",
    "        newj = 3\n",
    "    elif i==3 and j==2 and action==0:\n",
    "        newi = 3\n",
    "        newj = 3\n",
    "    #corners\n",
    "    # at row=0, cannot move upwards\n",
    "    elif i==0 and action==2:\n",
    "        newi = i\n",
    "        newj = j\n",
    "    # at row=4, cannot move downwards\n",
    "    elif i==3 and action==3:\n",
    "        newi = i\n",
    "        newj = j\n",
    "    # at column=0, cannot move left\n",
    "    elif j==0 and action==1:\n",
    "        newi = i\n",
    "        newj = j\n",
    "    # at column=4, cannot move right\n",
    "    elif j==3 and action==0:\n",
    "        newi = i\n",
    "        newj = j\n",
    "    else:\n",
    "        # move right\n",
    "        if action==0:\n",
    "            newi = i\n",
    "            newj = j+1\n",
    "        # move left\n",
    "        elif action==1:\n",
    "            newi = i\n",
    "            newj = j-1\n",
    "        # move upwards\n",
    "        elif action==2:\n",
    "            newi = i-1\n",
    "            newj = j\n",
    "        # move downwards\n",
    "        elif action==3:\n",
    "            newi = i+1\n",
    "            newj = j\n",
    "            \n",
    "    return newi, newj, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb0f49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "State-Value Function: \n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "Policy: \n",
      "[[0 1 1 1]\n",
      " [2 1 1 3]\n",
      " [2 2 0 3]\n",
      " [2 0 0 0]]\n",
      "Iteration: 2\n",
      "State-Value Function: \n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Policy: \n",
      "[[0 1 1 1]\n",
      " [2 1 0 3]\n",
      " [2 0 0 3]\n",
      " [0 0 0 0]]\n",
      "Iteration: 3\n",
      "State-Value Function: \n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Policy: \n",
      "[[0 1 1 1]\n",
      " [2 1 0 3]\n",
      " [2 0 0 3]\n",
      " [0 0 0 0]]\n",
      "Optimal state-value function: \n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Optimal policy: \n",
      "[[0 1 1 1]\n",
      " [2 1 0 3]\n",
      " [2 0 0 3]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing number of states, number of actions, gridSize and gamma\n",
    "num_states = 14\n",
    "num_actions = 4\n",
    "gridSize = 4\n",
    "threshold = pow(10,-13)\n",
    "\n",
    "# vpi represents the value function\n",
    "# piopt represents the policy\n",
    "vpi = np.zeros((gridSize,gridSize),dtype='float')\n",
    "piopt = np.ones((gridSize,gridSize,num_actions))\n",
    "piopt /= float(num_actions)\n",
    "\n",
    "iteration = 1\n",
    "while True:\n",
    "    print(\"Iteration: \"+str(iteration))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(0,gridSize):\n",
    "            for j in range(0,gridSize):\n",
    "                # if in terminating state continue\n",
    "                if i==0 and j==0:\n",
    "                    continue\n",
    "                if i==3 and j==3:\n",
    "                    continue\n",
    "                # newValFun represents summation over all actions, \n",
    "                # p(s',r|s,a)*(reward received on choosing action and moving to state s' + vpi[s'])\n",
    "                newValFun = 0.\n",
    "                currValFun = vpi[i,j]\n",
    "                for action in range(0,num_actions):\n",
    "                    # new state (newi,newj) obtained after moving from state (i,j) under action and reward represents the \n",
    "                    # corresponding reward\n",
    "                    newi,newj,reward = moving(i,j,action)\n",
    "                    newValFun += piopt[i,j,action] * (reward + vpi[newi,newj])\n",
    "                vpi[i,j] = newValFun\n",
    "\n",
    "                delta = max(delta, abs(currValFun - newValFun))\n",
    "        if delta<threshold:\n",
    "            break\n",
    "            \n",
    "    print(\"State-Value Function: \")\n",
    "    print(vpi)\n",
    "    \n",
    "    policy_stable = 1\n",
    "    for i in range(0,gridSize):\n",
    "        for j in range(0,gridSize):\n",
    "            # if in terminating state continue\n",
    "            if i==0 and j==0:\n",
    "                continue\n",
    "            if i==3 and j==3:\n",
    "                continue\n",
    "            currAction = np.random.choice(num_actions,p=piopt[i,j])\n",
    "            # expReturn for each action, represents reward received on choosing action and moving to state s' + vpi[s']\n",
    "            expReturn = np.zeros(num_actions)\n",
    "            for action in range(0,num_actions):\n",
    "                newi,newj,reward = moving(i,j,action)\n",
    "                expReturn[action] = reward + vpi[newi,newj]\n",
    "            optAction = np.argmax(expReturn)\n",
    "            for k in range(0,4):\n",
    "                piopt[i,j,k] = 0\n",
    "            piopt[i,j,optAction] = 1\n",
    "            if currAction != optAction:\n",
    "                policy_stable = 0\n",
    "    print(\"Policy: \")\n",
    "    print(np.argmax(piopt, axis=2))\n",
    "    if policy_stable==1:\n",
    "        break\n",
    "    iteration += 1\n",
    "\n",
    "print(\"Optimal state-value function: \")\n",
    "print(vpi)\n",
    "print(\"Optimal policy: \")\n",
    "print(np.argmax(piopt, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a1a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "State-Value Function: \n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "Iteration: 2\n",
      "State-Value Function: \n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "Iteration: 3\n",
      "State-Value Function: \n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Iteration: 4\n",
      "State-Value Function: \n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Optimal state-value function: \n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Optimal policy: \n",
      "[[0. 1. 1. 1.]\n",
      " [2. 1. 0. 3.]\n",
      " [2. 0. 0. 3.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing number of states, number of actions, gridSize and gamma\n",
    "num_states = 14\n",
    "num_actions = 4\n",
    "gridSize = 4\n",
    "threshold = pow(10,-13)\n",
    "\n",
    "# vpi represents the value function\n",
    "# piopt represents the policy\n",
    "vpi = np.zeros((gridSize,gridSize))\n",
    "piopt = np.zeros((gridSize,gridSize))\n",
    "\n",
    "iteration = 1\n",
    "while True:\n",
    "    print(\"Iteration: \"+str(iteration))\n",
    "    delta = 0\n",
    "    \n",
    "    for i in range(0,gridSize):\n",
    "        for j in range(0,gridSize):\n",
    "            # if in terminating state continue\n",
    "            if i==0 and j==0:\n",
    "                continue\n",
    "            if i==3 and j==3:\n",
    "                continue\n",
    "            currValFun = vpi[i,j]\n",
    "            # maxcoeff represents maximum over all actions, reward obtained from choosing action to get new state s'\n",
    "            # + vpi(s')\n",
    "            maxcoeff = -1000.\n",
    "            for action in range(0,num_actions):\n",
    "                newi,newj,reward = moving(i,j,action)\n",
    "                tempstorage = reward + vpi[newi,newj]\n",
    "                maxcoeff = max(maxcoeff, tempstorage)\n",
    "            vpi[i,j] = maxcoeff\n",
    "            delta = max(delta, abs(currValFun - maxcoeff))\n",
    "            \n",
    "    print(\"State-Value Function: \")\n",
    "    print(vpi)\n",
    "    if delta < threshold :\n",
    "        break\n",
    "    iteration += 1\n",
    "    \n",
    "for i in range(0,gridSize):\n",
    "    for j in range(0,gridSize):\n",
    "        # if in terminating state continue\n",
    "        if i==0 and j==0:\n",
    "            continue\n",
    "        if i==3 and j==3:\n",
    "            continue\n",
    "        # expReturn for each action, represents reward received on choosing action and moving to state s' + vpi[s']\n",
    "        expReturn = np.zeros(num_actions)\n",
    "        for action in range(0,num_actions):\n",
    "            newi,newj,reward = moving(i,j,action)\n",
    "            expReturn[action] = reward + vpi[newi,newj]\n",
    "        piopt[i,j] = np.argmax(expReturn)\n",
    "        \n",
    "print(\"Optimal state-value function: \")\n",
    "print(vpi)\n",
    "print(\"Optimal policy: \")\n",
    "print(piopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06432f",
   "metadata": {},
   "source": [
    "Since argmax resolves ties by selecting the smallest action (if action 1 and 3 both are maximizing policy then argmax picks action 1), the bug mentioned in exercise 4.4 does not arise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
